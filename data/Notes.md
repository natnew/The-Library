# ML Papers - Podacst - Learning on the Go

#### LargeConceptModels: LanguageModelinginaSentenceRepresentationSpac 
Source: https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/

#### Automating the Search for Artificial Life with Foundation Models
Source: https://arxiv.org/pdf/2412.17799

#### ExploreTheory-of-Mind: Program-GuidedAdversarial DataGenerationforTheoryofMindReasoning
Source: https://ai.meta.com/research/publications/explore-theory-of-mind-program-guided-adversarial-data-generation-for-theory-of-mind-reasoning/

####  LearnLM: Improving Gemini for Learning
Source: https://services.google.com/fh/files/misc/improving-gemini-for-education_v7.pdf

#### DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought
Source: https://arxiv.org/pdf/2412.17498

#### Qwen-2.5 Technical Report
Source: https://arxiv.org/pdf/2412.15115

#### PAE (Proposer-Agent-Evaluator)
Source: https://arxiv.org/pdf/2412.13194

#### DeepSeek-VL2
Source: https://arxiv.org/pdf/2412.10302

#### ALIGNMENT FAKING IN LARGE LANGUAGE MODELS
Source: https://arxiv.org/pdf/2412.14093

#### THEAGENTCOMPANY: BENCHMARKING LLM AGENTS ON CONSEQUENTIAL REAL WORLD TASKS
Source: https://arxiv.org/pdf/2412.14161

#### Training Large Language Models to Reason in a Continuous Latent Space
Source: https://arxiv.org/pdf/2412.06769

#### Phi-4 Technical Report
Source: https://arxiv.org/pdf/2412.08905

#### ASYNCHRONOUS LLM FUNCTION CALLING
Source: https://arxiv.org/pdf/2412.07017

#### Clio: Privacy-Preserving Insights into Real-World AI Use
Source: https://assets.anthropic.com/m/7e1ab885d1b24176/original/Clio-Privacy-Preserving-Insights-into-Real-World-AI-Use.pdf

#### AUTOREASON: AUTOMATIC FEW-SHOT REASONING DECOMPOSITION
Source: https://arxiv.org/pdf/2412.06975

#### OpenAI o1 System Card
Source:https://cdn.openai.com/o1-system-card-20241205.pdf

#### Genie 2: A large-scale foundation world model
Source:https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/

#### Reverse Thinking Makes LLMs Stronger Reasoners
Source: https://arxiv.org/pdf/2411.19865

#### Towards Adaptive Mechanism Activation in Language Agent
Source: https://arxiv.org/pdf/2412.00722

#### AUTO-RAG: AUTONOMOUS RETRIEVAL-AUGMENTED GENERATION FOR LARGE LANGUAGE MODELS
Source: https://arxiv.org/pdf/2411.19443

#### Challenges in Human-Agent Communication
Source: https://www.microsoft.com/en-us/research/uploads/prod/2024/12/HCAI_Agents.pdf

#### RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models
Source: https://arxiv.org/pdf/2412.02830

#### DataLab: A Unified Platform for LLM-Powered Business Intelligence
Source: https://arxiv.org/pdf/2412.02205

#### PROCEDURAL KNOWLEDGE IN PRETRAINING DRIVES REASONING IN LARGE LANGUAGE MODELS
Source: https://arxiv.org/pdf/2411.12580

#### Byte Latent Transformer: Patches Scale Better Than Tokens
Source: https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/

#### DOES RLHF SCALE? EXPLORING THE IMPACTS FROM DATA, MODEL, AND METHOD
Source: https://arxiv.org/pdf/2412.06000

#### Large language models surpass human experts in predicting neuroscience results
Source: https://www.nature.com/articles/s41562-024-02046-9

#### Large Language Model-Brained GUI Agents: A Survey
Source:https://arxiv.org/pdf/2411.18279

#### Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS
Source: https://arxiv.org/pdf/2411.18478

#### TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training
Source: https://arxiv.org/pdf/2411.15124

#### Generative Agent Simulations of 1,000 People
Source:https://arxiv.org/pdf/2411.10109

#### Scaling Laws for Precision
Source: https://arxiv.org/pdf/2411.04330

#### Artificial Intelligence, Scientific Discovery, and Product Innovation
Source: https://aidantr.github.io/files/AI_innovation.pdf

####  OPENCODER: THE OPEN COOKBOOK FOR TOP-TIER CODE LARGE LANGUAGE MODELS
Source: https://arxiv.org/pdf/2411.04905

#### The Surprising Effectiveness of Test-Time Training for Abstract Reasoning
Source: https://ekinakyurek.github.io/papers/ttt.pdf

#### ATaxonomy of AgentOps for Enabling Observability of Foundation Model based Agents
Source: https://arxiv.org/pdf/2411.05285v1

#### Toward Optimal Search and Retrieval for RAG
Source: https://arxiv.org/pdf/2411.07396

#### RAPID RESPONSE: MITIGATING LLM JAILBREAKS WITH A FEW EXAMPLES
Source:https://arxiv.org/pdf/2411.07494

#### Project Sid: Many-agent simulations toward AI civilization
Source:https://arxiv.org/pdf/2411.00114

#### Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models
Source: https://arxiv.org/pdf/2411.00492

#### Number Cookbook: Number Understanding of Language Models and How to Improve It
Source: https://arxiv.org/pdf/2411.03766

#### WEBRL: TRAINING LLM WEB AGENTS VIA SELF EVOLVING ONLINE CURRICULUM REINFORCEMENT LEARNING
Source: https://arxiv.org/pdf/2411.02337

#### Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation
Source: https://arxiv.org/pdf/2411.00412

#### Personalization of Large Language Models: A Survey
Source: https://arxiv.org/pdf/2411.00027

#### THE GEOMETRY OF CONCEPTS: SPARSE AUTOENCODER FEATURE STRUCTURE
Source: https://arxiv.org/pdf/2410.19750

#### AFLOW: AUTOMATING AGENTIC WORKFLOW GENERATION
Source: https://arxiv.org/pdf/2410.10762

#### ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS
Source: https://arxiv.org/pdf/2410.21272

#### Distinguishing Ignorance from Error in LLM Hallucinations
Source: https://arxiv.org/pdf/2410.22071

#### Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial
 Applications
Source: https://arxiv.org/pdf/2410.21943

#### Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models
Source: https://arxiv.org/pdf/2410.19385

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source:

####
Source: