# flashcards.py

flashcards = [
    {
        "id": 1,
        "topic": "Semantic/Vector Search",
        "answer": (
            "Semantic search, also called vector search, uses machine learning "
            "to analyse text context beyond keywords. It converts text into "
            "numerical vectors, enabling searches based on conceptual similarity "
            "for highly relevant results."
        ),
    },
    {
        "id": 2,
        "topic": "Keyword Search",
        "answer": (
            "Keyword search identifies exact matches between query terms and "
            "document content. It uses the BM25/F function to calculate relevance "
            "by analysing term frequency and inverse document frequency."
        ),
    },
    {
        "id": 3,
        "topic": "Hybrid Search",
        "answer": (
            "Hybrid search merges keyword-based search (BM25) and vector search "
            "to balance exact term matching with semantic understanding. It "
            "leverages fusion algorithms to combine results, providing both "
            "precision and relevance."
        ),
    },
    {
        "id": 4,
        "topic": "Fusion Algorithm",
        "answer": (
            "Fusion algorithms combine results from keyword-based and vector searches "
            "to create a final ranked list. A common approach, Reciprocal Rank Fusion, "
            "calculates rankings based on the sum of the inverse positions of results. "
            "Other fusion methods, such as relative score fusion, are also supported."
        ),
    },
    {
        "id": 5,
        "topic": "Alpha Parameter",
        "answer": (
            "The alpha parameter is used in hybrid search queries to balance the "
            "weights between sparse (BM25) and dense vectors. An alpha value of 0 "
            "gives full weight to sparse vectors, 1 favours dense vectors, and 0.5 "
            "balances both equally."
        ),
    },
    {
        "id": 6,
        "topic": "BM25/BM25F",
        "answer": (
            "BM25 is a ranking function that evaluates document relevance for a given "
            "query. It is part of probabilistic information retrieval models. BM25F "
            "extends BM25 by handling multiple fields with different weights, such as "
            "document titles and bodies."
        ),
    },
    {
        "id": 7,
        "topic": "Dense Vectors",
        "answer": (
            "Dense vectors are low-dimensional embeddings with mostly non-zero values. "
            "Generated by machine learning models such as GloVe and Transformers, they "
            "capture the semantic meaning of text and are widely used in vector search "
            "techniques."
        ),
    },
    {
        "id": 8,
        "topic": "Sparse Vectors",
        "answer": (
            "Sparse vectors are high-dimensional embeddings that often contain many "
            "zero values. They are generated using algorithms like BM25 and SPLADE "
            "to represent text documents in keyword-based searches, enabling "
            "effective retrieval."
        ),
    },
    {
        "id": 9,
        "topic": "Retrieval Augmented Generation (RAG)",
        "answer": (
            "Retrieval Augmented Generation (RAG) combines retrieval and generation "
            "by contextualising prompts for large language models (LLMs). It "
            "retrieves relevant information from a data store and incorporates it "
            "into the LLM's generation process, enabling more informed and "
            "accurate predictions."
        ),
    },
    {
        "id": 10,
        "topic": "Re-Ranking",
        "answer": (
            "Re-ranking is the process of adjusting search result scores after the "
            "initial retrieval. Machine learning models reassess the relevance of each "
            "result to the query and refine the scores to enhance accuracy and "
            "relevance. It acts as a quality check to ensure the user or an LLM "
            "receives the most relevant information."
        ),
    },
    {
        "id": 11,
        "topic": "Transformer Model",
        "answer": (
            "The transformer model is a neural network architecture designed for "
            "sequential data, such as text. It excels at tasks like translation and "
            "question-answering by using attention mechanisms to assign importance to "
            "different parts of input data, ensuring scalability and efficiency."
        ),
    },
    {
        "id": 12,
        "topic": "Generative AI",
        "answer": (
            "Generative AI refers to machine learning models that create original "
            "outputs—such as text, images, audio, or video—based on user-provided "
            "prompts. These models are capable of generating novel content across "
            "multiple modalities."
        ),
    },
    {
        "id": 13,
        "topic": "Chunking",
        "answer": (
            "Chunking is the process of breaking large texts into smaller, manageable "
            "pieces called 'chunks.' This approach improves information retrieval in "
            "vector databases and enhances search performance and model output quality."
        ),
    },
    {
        "id": 14,
        "topic": "Embedding Model",
        "answer": (
            "An embedding model transforms text or images into vector embeddings—"
            "numerical representations that capture semantic meaning. By analysing "
            "relationships between inputs, these embeddings enable models to process "
            "and understand data efficiently."
        ),
    },
    {
        "id": 15,
        "topic": "Multi-Modal",
        "answer": (
            "Multimodal learning integrates different types of data—such as images, "
            "text, audio, and sensory inputs—to develop models that process and "
            "understand information like humans. It identifies patterns across "
            "various input types for diverse use cases, such as e-commerce or "
            "content recommendation."
        ),
    },
    {
        "id": 16,
        "topic": "Fine-Tuning",
        "answer": (
            "Fine-tuning is the process of further training a pre-trained machine "
            "learning model on a specific dataset. This approach enhances the model's "
            "knowledge or performance for a particular task or domain, making it more "
            "efficient than training from scratch."
        ),
    },
    {
        "id": 17,
        "topic": "Large Language Model (LLM)",
        "answer": (
            "A Large Language Model (LLM) is a machine learning model trained on "
            "extensive text data. It learns patterns in language to understand, "
            "predict, and generate human-like text. By statistically modelling "
            "language, LLMs can produce context-aware responses to queries."
        ),
    },
    {
        "id": 18,
        "topic": "Multimodal RAG (Retrieval-Augmented Generation)",
        "answer": (
            "Multimodal RAG retrieves information from a multimodal knowledge base "
            "and generates output—text, images, or other modalities—using a large "
            "multimodal model. The generation is grounded in retrieved data, "
            "combining insights from images, text, audio, and more."
        ),
    },
    {
        "id": 19,
        "topic": "Any-to-Any Search",
        "answer": (
            "Any-to-any search allows querying the model using any modality (e.g., "
            "image, audio, text) to retrieve relevant objects from a different "
            "modality. The model uses vector similarity search in a multimodal "
            "embedding space to identify conceptually similar items."
        ),
    },
    {
        "id": 20,
        "topic": "Multimodal Contrastive Fine-Tuning",
        "answer": (
            "Multimodal contrastive fine-tuning enhances the embedding space by "
            "aligning similar objects across modalities (e.g., text and images) while "
            "pushing dissimilar ones apart. This fine-tuning step helps models preserve "
            "relationships and semantic similarities within multimodal data."
        ),
    },
    {
        "id": 21,
        "topic": "Multimodal Embedding Models",
        "answer": (
            "Multimodal embedding models create a unified embedding space that represents "
            "data across multiple modalities, such as text, images, and audio. Similar "
            "items are placed closer together, while dissimilar ones are farther apart. "
            "This ensures that semantic meaning is preserved both within and across "
            "different data types."
        ),
    },
    {
        "id": 22,
        "topic": "Cross-Modal Reasoning",
        "answer": (
            "Cross-modal reasoning enables connections between different types of "
            "information, such as text, images, audio, or video. It involves using "
            "insights from one modality to analyse or understand data from another "
            "modality, enhancing the ability to reason across diverse sources."
        ),
    },
]
